{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable plotting graphs in Jupyter notebook\n",
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "#Let us break the X and y dataframes into training set and test set. For this we will use\n",
    "#Sklearn package's data splitting function which is based on random function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "\n",
    "# calculate accuracy measures and confusion matrix\n",
    "#from sklearn import metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       preg  plas  pres  skin  test  mass  pedi  age\n",
       "class                                               \n",
       "0       500   500   500   500   500   500   500  500\n",
       "1       268   268   268   268   268   268   268  268"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it is a data file with no header, we will supply the column names which have been obtained from the above URL \n",
    "# Create a python list of column names called \"names\"\n",
    "\n",
    "colnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "#Load the file from local directory using pd.read_csv which is a special form of read_table\n",
    "#while reading the data, supply the \"colnames\" list\n",
    "\n",
    "pima_df = pd.read_csv(\"pima-indians-diabetes.data\", names= colnames)\n",
    "pima_df.groupby([\"class\"]).count()  # count the instances of each class in the data to check if data is skewed towards a class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the missing values in pima_df with median value :Note, we do not need to specify the column names\n",
    "# every column's missing value is replaced with that column's median respectively\n",
    "pima_df = pima_df.fillna(pima_df.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = pima_df.values\n",
    "X = array[:,0:7] # select all rows and first 8 columns which are the attributes\n",
    "Y = array[:,8]   # select all rows and the 8th column which is the classification \"Yes\", \"No\" for diabeties\n",
    "test_size = 0.30 # taking 70:30 training and test set\n",
    "seed = 7  # Random numbmer seeding for reapeatability of the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt_model = DecisionTreeClassifier(criterion = 'entropy', class_weight={0:.80,1:.20}, max_depth = 9, min_samples_leaf=5 )\n",
    "dt_model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5)\n",
    "# When the weights are changed in favor of class 1 (diabetic) because their instances are half that in the other class, the \n",
    "# accuracy drops.... Why?\n",
    "\n",
    "# When the weights are changee to favor the 0 class, the accuracy improves. ... Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcl = LogisticRegression(random_state=1)\n",
    "rfcl = RandomForestClassifier(random_state=1)\n",
    "nbcl = GaussianNB()\n",
    "bgcl = BaggingClassifier(base_estimator=dt_model, n_estimators=50 , random_state=1)  \n",
    "#the base_estimator can be null. The bagging classifer  will build it's own tree\n",
    "\n",
    "enclf = VotingClassifier(estimators = [('lor', lrcl), ('rf', rfcl), ('nb', nbcl), ('bg', bgcl)], voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.02) [Logistic Regression]\n",
      "Accuracy: 0.74 (+/- 0.02) [RandomForest]\n",
      "Accuracy: 0.77 (+/- 0.02) [NaiveBayes]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mukesh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76 (+/- 0.02) [Ensemble]\n",
      "Accuracy: 0.75 (+/- 0.03) [Bagging]\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip([lrcl , rfcl, nbcl, enclf, bgcl], ['Logistic Regression', 'RandomForest', 'NaiveBayes', 'Ensemble', 'Bagging']):\n",
    "    scores = cross_val_score(clf, X, Y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.02f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyper parameters {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'max_iter': 100, 'multi_class': 'warn', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1, 'solver': 'warn', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "hyper parameters {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "hyper parameters {'priors': None, 'var_smoothing': 1e-09}\n",
      "hyper parameters {'estimators': [('lor', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=1, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False)), ('nb', GaussianNB(priors=None, var_smoothing=1e-09)), ('bg', BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=1.0, n_estimators=50, n_jobs=None, oob_score=False,\n",
      "         random_state=1, verbose=0, warm_start=False))], 'flatten_transform': None, 'n_jobs': None, 'voting': 'hard', 'weights': None, 'lor': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=1, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), 'rf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'nb': GaussianNB(priors=None, var_smoothing=1e-09), 'bg': BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=1.0, n_estimators=50, n_jobs=None, oob_score=False,\n",
      "         random_state=1, verbose=0, warm_start=False), 'lor__C': 1.0, 'lor__class_weight': None, 'lor__dual': False, 'lor__fit_intercept': True, 'lor__intercept_scaling': 1, 'lor__max_iter': 100, 'lor__multi_class': 'warn', 'lor__n_jobs': None, 'lor__penalty': 'l2', 'lor__random_state': 1, 'lor__solver': 'warn', 'lor__tol': 0.0001, 'lor__verbose': 0, 'lor__warm_start': False, 'rf__bootstrap': True, 'rf__class_weight': None, 'rf__criterion': 'gini', 'rf__max_depth': None, 'rf__max_features': 'auto', 'rf__max_leaf_nodes': None, 'rf__min_impurity_decrease': 0.0, 'rf__min_impurity_split': None, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__min_weight_fraction_leaf': 0.0, 'rf__n_estimators': 'warn', 'rf__n_jobs': None, 'rf__oob_score': False, 'rf__random_state': 1, 'rf__verbose': 0, 'rf__warm_start': False, 'nb__priors': None, 'nb__var_smoothing': 1e-09, 'bg__base_estimator__class_weight': None, 'bg__base_estimator__criterion': 'entropy', 'bg__base_estimator__max_depth': 5, 'bg__base_estimator__max_features': None, 'bg__base_estimator__max_leaf_nodes': None, 'bg__base_estimator__min_impurity_decrease': 0.0, 'bg__base_estimator__min_impurity_split': None, 'bg__base_estimator__min_samples_leaf': 1, 'bg__base_estimator__min_samples_split': 2, 'bg__base_estimator__min_weight_fraction_leaf': 0.0, 'bg__base_estimator__presort': False, 'bg__base_estimator__random_state': None, 'bg__base_estimator__splitter': 'best', 'bg__base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'bg__bootstrap': True, 'bg__bootstrap_features': False, 'bg__max_features': 1.0, 'bg__max_samples': 1.0, 'bg__n_estimators': 50, 'bg__n_jobs': None, 'bg__oob_score': False, 'bg__random_state': 1, 'bg__verbose': 0, 'bg__warm_start': False}\n",
      "hyper parameters {'base_estimator__class_weight': None, 'base_estimator__criterion': 'entropy', 'base_estimator__max_depth': 5, 'base_estimator__max_features': None, 'base_estimator__max_leaf_nodes': None, 'base_estimator__min_impurity_decrease': 0.0, 'base_estimator__min_impurity_split': None, 'base_estimator__min_samples_leaf': 1, 'base_estimator__min_samples_split': 2, 'base_estimator__min_weight_fraction_leaf': 0.0, 'base_estimator__presort': False, 'base_estimator__random_state': None, 'base_estimator__splitter': 'best', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip([lrcl , rfcl, nbcl, enclf, bgcl], ['Logistic Regression', 'RandomForest', 'NaiveBayes', 'Ensemble', 'Bagging']):\n",
    "    print(\"hyper parameters\" , clf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
