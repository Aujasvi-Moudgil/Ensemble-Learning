{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter. \n",
    "- The tree can be explained by two entities, namely decision nodes and leaves. The leaves are the decisions or the final outcomes.\n",
    "- And the decision nodes are where the data is split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An example of a decision tree can be:-\n",
    "- Let’s say you want to predict whether a person is fit given their information like age, eating habit, and physical activity, etc.\n",
    "- The decision nodes here are questions like \n",
    "  - ‘What’s the age?’, \n",
    "  - ‘Does he exercise?’,\n",
    "  - ‘Does he eat a lot of pizzas’?\n",
    "- And the leaves,\n",
    "  - which are outcomes like either ‘fit’, or ‘unfit’. \n",
    "- In this case this was a binary classification problem (a yes no type problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are two main types of Decision Trees:\n",
    "\n",
    "- Classification trees (Yes/No types)\n",
    "  - What we’ve seen above is an example of classification tree, where the outcome was a variable like ‘fit’ or ‘unfit’.\n",
    "  - Here the decision variable is Categorical.\n",
    "\n",
    "- Regression trees (Continuous data types)\n",
    "  - Here the decision or the outcome variable is Continuous, e.g. a number like 123."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Introduction to Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensemble modeling is a powerful way to improve the performance of your model.\n",
    "- It usually pays off to apply ensemble learning over and above various models you might be building.\n",
    "- Time and again, people have used ensemble models in competitions like Kaggle and benefited from it.\n",
    "- Ensemble learning is a broad topic and is only confined by your own imagination. \n",
    "- Let’s start with an example to understand the basics of Ensemble learning.\n",
    "- This example will bring out, how we use ensemble model every day without realizing that we are using ensemble modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "- I want to invest in a company XYZ. \n",
    "- I am not sure about its performance though.\n",
    "- So, I look for advice on whether the stock price will increase more than 6% per annum or not? \n",
    "- I decide to approach various experts having diverse domain experience:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.Employee of Company XYZ**: \n",
    "  - This person knows the internal functionality of the company and have the insider information about the functionality of the firm.\n",
    "  - But he lacks a broader perspective on how are competitors innovating, how is the technology evolving and what will be the impact of this evolution on Company XYZ’s product. \n",
    "  - **In the past, he has been right 70% times.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2. Financial Advisor of Company XYZ:**\n",
    "  - This person has a broader perspective on how companies strategy will fair of in this competitive environment.\n",
    "  - However, he lacks a view on how the company’s internal policies are fairing off. \n",
    "  - **In the past, he has been right 75% times.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3. Stock Market Trader:**\n",
    "  - This person has observed the company’s stock price over past 3 years. \n",
    "  - He knows the seasonality trends and how the overall market is performing.\n",
    "  - He also has developed a strong intuition on how stocks might vary over time. \n",
    "  - **In the past, he has been right 70% times.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **4. Employee of a competitor:**\n",
    "  - This person knows the internal functionality of the competitor firms and is aware of certain changes which are yet to be brought.\n",
    "  - He lacks a sight of company in focus and the external factors which can relate the growth of competitor with the company of subject.\n",
    "  - **In the past, he has been right  60% of times.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **5. Market Research team in same segment:**\n",
    "  - This team analyzes the customer preference of company XYZ’s product over others and how is this changing with time.\n",
    "  - Because he deals with customer side, he is unaware of the changes company XYZ will bring because of alignment to its own goals. \n",
    "  - **In the past, they have been right 75% of times.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **6. Social Media Expert:**\n",
    "  - This person can help us understand how has company XYZ has positioned its products in the market. \n",
    "  - And how are the sentiment of customers changing over time towards company. He is unaware of any kind of details beyond digital marketing.\n",
    "  - **In the past, he has been right 65% of times.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Ensemble is the art of combining diverse set of learners (individual models) together to improvise on the stability and predictive power of the model.**\n",
    "- In the above example, **the way we combine all the predictions together will be termed as Ensemble Learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Balance Scale Weight & Distance Database\n",
    "- **Number of Instances:** 625 (49 balanced, 288 left, 288 right)\n",
    "- **Number of Attributes:** 4 (numeric) + class name = 5\n",
    "- **Attribute Information:**\n",
    "- **Class Name (Target variable):** 3\n",
    "  - L [balance scale tip to the left]\n",
    "  - B [balance scale be balanced]\n",
    "  - R [balance scale tip to the right]\n",
    "- Left-Weight: 5 (1, 2, 3, 4, 5)\n",
    "- Left-Distance: 5 (1, 2, 3, 4, 5)\n",
    "- Right-Weight: 5 (1, 2, 3, 4, 5)\n",
    "- Right-Distance: 5 (1, 2, 3, 4, 5)\n",
    "\n",
    "- **Missing Attribute Values:** None\n",
    "- **Class Distribution:**\n",
    "- 46.08 percent are L\n",
    "- 07.84 percent are B\n",
    "- 46.08 percent are R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assumptions we make while using Decision tree :\n",
    "- At the beginning, we consider the whole training set as the root.\n",
    "- Attributes are assumed to be categorical for information gain and for gini index, attributes are assumed to be continuous.\n",
    "- On the basis of attribute values records are distributed recursively.\n",
    "- We use statistical methods for ordering attributes as root or internal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode :\n",
    "- Find the best attribute and place it on the root node of the tree.\n",
    "- Now, split the training set of the dataset into subsets. While making the subset make sure that each subset of training dataset should have the same value for an attribute.\n",
    "- Find leaf nodes in all branches by repeating 1 and 2 on each subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While implementing the decision tree we will go through the following two phases:\n",
    "\n",
    "- **Building Phase**\n",
    "  - Preprocess the dataset.\n",
    "  - Split the dataset from train and test using Python sklearn package.\n",
    "  - Train the classifier.\n",
    "- **Operational Phase**\n",
    "  - Make predictions.\n",
    "  - Calculate the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn :\n",
    "- In python, sklearn is a machine learning package which include a lot of ML algorithms.\n",
    "- Here, we are using some of its modules like train_test_split, DecisionTreeClassifier and accuracy_score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy :\n",
    "- It is a numeric python module which provides fast maths functions for calculations.\n",
    "- It is used to read data in numpy arrays and for manipulation purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas :\n",
    "- Used to read and write different files.\n",
    "- Data manipulation can be done easily with dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms used in code :\n",
    "- Gini index and information gain both of these methods are used to select from the n attributes of the dataset which attribute would be placed at the root node or the internal node.\n",
    "- **Gini Index** is a metric to measure how often a randomly chosen element would be incorrectly identified.\n",
    "- It means an attribute with lower gini index should be preferred.\n",
    "- Sklearn supports “gini” criteria for Gini Index and by default, it takes “gini” value.\n",
    "\n",
    "\n",
    "- **Entropy**\n",
    "  - Entropy is the measure of uncertainty of a random variable, it characterizes the impurity of an arbitrary collection of examples. The higher the entropy the more the information content.\n",
    "- **Information Gain**\n",
    "  - The entropy typically changes when we use a node in a decision tree to partition the training instances into smaller subsets. Information gain is a measure of this change in entropy.\n",
    "- Sklearn supports “entropy” criteria for Information Gain and if we want to use Information Gain method in sklearn then we have to mention it explicitly.\n",
    "- **Accuracy score**\n",
    "  - Accuracy score is used to calculate the accuracy of the trained classifier.\n",
    "- **Confusion Matrix**\n",
    "  - Confusion Matrix is used to understand the trained classifier behavior over the test dataset or validate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aniruddhakalbande/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing the required packages \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_data = pd.read_csv( 'https://archive.ics.uci.edu/ml/machine-learning-'+'databases/balance-scale/balance-scale.data',sep= ',', header = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0  B  1  1  1  1\n",
       "1  R  1  1  1  2\n",
       "2  R  1  1  1  3\n",
       "3  R  1  1  1  4\n",
       "4  R  1  1  1  5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Lenght:  625\n",
      "Dataset Shape:  (625, 5)\n"
     ]
    }
   ],
   "source": [
    "# Printing the dataswet shape \n",
    "print (\"Dataset Lenght: \", len(balance_data)) \n",
    "print (\"Dataset Shape: \", balance_data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Seperating the target variable\n",
    "X = balance_data.values[:, 1:5] \n",
    "Y = balance_data.values[:, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the dataset into train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 100) \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training with giniIndex. \n",
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\",random_state = 100,max_depth=3, min_samples_leaf=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing training \n",
    "clf_gini.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree with entropy \n",
    "clf_entropy = DecisionTreeClassifier( criterion = \"entropy\", random_state = 100,max_depth = 3, min_samples_leaf = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing training \n",
    "clf_entropy.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "['R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L'\n",
      " 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L'\n",
      " 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R'\n",
      " 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L'\n",
      " 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L'\n",
      " 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R'\n",
      " 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R'\n",
      " 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R'\n",
      " 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R']\n"
     ]
    }
   ],
   "source": [
    "# Predicton on test with giniIndex \n",
    "y_pred = clf_gini.predict(X_test) \n",
    "print(\"Predicted values:\") \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "['R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L'\n",
      " 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L'\n",
      " 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L'\n",
      " 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L'\n",
      " 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L'\n",
      " 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R'\n",
      " 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R'\n",
      " 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R']\n"
     ]
    }
   ],
   "source": [
    "# Predicton on test with entropy \n",
    "y_pred_2 = clf_entropy.predict(X_test) \n",
    "print(\"Predicted values:\") \n",
    "print(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  6  7]\n",
      " [ 0 67 18]\n",
      " [ 0 19 71]]\n"
     ]
    }
   ],
   "source": [
    "# GiniIndex\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  6  7]\n",
      " [ 0 63 22]\n",
      " [ 0 20 70]]\n"
     ]
    }
   ],
   "source": [
    "# Entropy\n",
    "print(confusion_matrix(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73404255319148937"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# giniIndex\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70744680851063835"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy\n",
    "accuracy_score(y_test,y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In class lab WAP : Use Decision Tree Classification Algorithm\n",
    "    Data Set Name: credit.csv ,Using the dataset, perform \n",
    "1. Decision Tree Classification Algorithm (Restricting the depth of the tree to 5)\n",
    "2. Using entropy and Gini Index Method\n",
    "3. Perform all the evaluation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take home assignment***\n",
    "\n",
    "    Data Set Name: Heart.csv ,Using the dataset, perform \n",
    "1. Decision Tree Classification Algorithm \n",
    "2. Using entropy and Gini Index Method\n",
    "3. Perform all the evaluation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
